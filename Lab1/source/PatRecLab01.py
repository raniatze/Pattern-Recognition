# -*- coding: utf-8 -*-
"""PatRecLab01.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YY983Wmy1AAGzLt5YDl0rGu7iVmmPivD

## Imports
"""

import numpy as np
import random
import matplotlib.pyplot as plt
from sklearn.base import BaseEstimator, ClassifierMixin
from sklearn.manifold import TSNE
from sklearn.decomposition import TruncatedSVD
from sklearn.svm import SVC
from sklearn.model_selection import learning_curve,cross_validate
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import VotingClassifier
from sklearn.ensemble import BaggingClassifier
from sklearn.metrics import plot_confusion_matrix

"""## Βήμα 1

Διαβάζουμε τα δεδομένα εκπαίδευσης και δοκιμής από τα αρχεία **train.txt** και **test.txt** αντίστοιχα. Τα δεδομένα θέλουμε να είναι σε μορφή συμβατή με το **scikit-learn**. Φροντίζουμε λοιπόν με κατάλληλο slicing ο πίνακας **X_train** να περιέχει τα δείγματα εκπαίδευσης (χωρίς τα
labels) και να είναι διάστασης **n_samples_train x n_features** ενώ ο **y_train** να είναι ένας μονοδιάστατος πίνακας μήκους
**n_samples** και να περιέχει τα αντίστοιχα labels για τον X_train. Ακριβώς αντίστοιχα δημιουργούμε και τους πίνακες **X_test** και **y_test** για τα δεδομένα δοκιμής.
"""

X_train_data = np.loadtxt("train.txt")
X_test_data = np.loadtxt("test.txt")

y_train = X_train_data[:,0].astype(int)
X_train = X_train_data[:,1:]

y_test = X_test_data[:,0].astype(int)
X_test = X_test_data[:,1:]

print("\033[1m   Train dataset     \033[0m \n")
print("X_train:",X_train.shape)
print("y_train:",y_train.shape)
print(" \n")
print("\033[1m   Test dataset     \033[0m \n")
print("X_test: ",X_test.shape)
print("y_test: ",y_test.shape)

"""## Βήμα 2

Ορίζουμε τη συνάρτηση **show_sample** η οποία δέχεται ως όρισμα ένα σύνολο δεδομένων X και έναν δείκτη index ενώ επιστρέφει μια εικόνα 16 x 16 του ψηφίου που αντιστοιχεί στη θέση index του συνόλου Χ. Αξιοποιούμε τη συνάρτηση **reshape** της numpy για να οργανώσουμε τα 256 χαρακτηριστικά σε ένα πίνακα 16 x 16 ενώ με τη χρήση της **imshow** της matplotlib απεικονίζουμε την τελική εικόνα.
"""

def show_sample(X, index):
    digit_index = X[index,:].reshape(16,16)
    plt.imshow(digit_index,cmap='gray')

show_sample(X_train,130)
plt.show()

"""## Bήμα 3

Ορίζουμε τώρα τη συνάρτηση **plot_digits_samples** η οποία δέχεται ως όρισμα ένα σύνολο δεδομένων Χ και ένα μονοδιάστατο πίνακα y που περιέχει τα labels του συνόλου Χ. 
Για κάθε ένα από τα 10 ψηφία (0,1,2,...,9) η συνάρτηση εντοπίζει τα indexes (θέσεις) που αυτό εμφανίζεται στον πίνακα y και τα αποθηκεύει σε έναν πίνακα. Αυτό πραγματοποιείται μέσω της συνάρτησης **argwhere** της βιβλιοθήκης numpy. Στη συνέχεια, από τον πίνακα αυτό επιλέγεται τυχαία μια τιμή (ένα index) καθώς μας ενδιαφέρει απλά ένα τυχαίο δείγμα από κάθε label. Αυτό πραγματοποιείται με τη βοήθεια της συνάρτησης **random.choice**. Μέσω της τιμής αυτής επιλέγεται η αντίστοιχη γραμμή του πίνακα X 
στην οποία είναι αποθηκευμένο το εν λόγω ψηφίο. Αξιοποιούμε τη συνάρτηση **reshape** της numpy για να οργανώσουμε τα 256 χαρακτηριστικά του σε ένα πίνακα 16 x 16 και έπειτα απεικονίζουμε την τελική εικόνα σε ένα subplot.
Τελικά η συνάρτηση επιστρέφει ένα subplot που περιέχει και τα 10 ψηφία.
"""

def plot_digits_samples(X, y):
    fig,axs = plt.subplots(1,10,figsize=(15,15))
    for digit in range(10):
        random_sample = X[random.choice(np.argwhere(y==digit)[:,0]),:].reshape(16,16)
        axs[digit].imshow(random_sample,cmap='gray')
        axs[digit].set_title("digit {}".format(digit))

plot_digits_samples(X_train,y_train)

"""## Βήμα 4

Θέλουμε τώρα να υπολογίσουμε τη μέση τιμή των χαρακτηριστικών του pixel (10,10) για το ψηφίο "0" με βάση τα train δεδομένα. 

Για το σκοπό αυτό ορίζουμε τη συνάρτηση **digit_mean_at_pixel**
η οποία δέχεται τέσσερα ορίσματα: 

*   ένα σύνολο δεδομένων X
*   ένα μονοδιάστατο πίνακα y με τα αντίστοιχα labels του Χ
*   ένα δεδομένο ψηφίο digit 
*   ένα tuple με τις συντεταγμένες του pixel, οι οποίες έχουν default τιμή (10,10)

Η συνάρτηση αρχικά υπολογίζει το index (0-255) που αντιστοιχεί στη θέση του pixel. Αυτό πραγματοποιείται πολλαπλασιάζοντας με 16 (σύνολο στηλών του πίνακα) το σύνολο όλων των γραμμών που υπάρχουν πριν από τη γραμμή που βρίσκεται το pixel και αθροίζοντας στο αποτέλεσμα το σύνολο των στηλών που βρίσκονται πριν από τη στήλη που βρίσκεται το pixel. Ο αριθμός αυτός μας υποδεικνύει τη στήλη του πίνακα X που πρόκειται να εξετάσουμε.
Μέσω της συνάρτησης **argwhere** της βιβλιοθήκης numpy εντοπίζουμε τις θέσεις του πίνακα y που περιέχουν το ψηφίο digit, και κατά συνέπεια τις γραμμές του πίνακα X που αφορούν το εν λόγω ψηφίο. 
Έτσι, αξιοποιώντας τώρα τη συνάρτηση **mean** της numpy λαμβάνουμε τη μέση τιμή όλων των τιμών που βρίσκονται στις γραμμές αυτές και στην προαναφερθείσα στήλη.
"""

def digit_mean_at_pixel(X, y, digit, pixel=(10, 10)):
    train_col = pixel[0]*16 + pixel[1]
    train_rows = np.argwhere(y==digit)[:,0]
    pixel_values = [X[train_row,train_col] for train_row in train_rows]
    return np.mean(pixel_values)

mean = digit_mean_at_pixel(X_train,y_train,0,(10,10))
print("Mean value of pixel (10,10) of digit 0 is:\033[1m",mean)

"""## Βήμα 5

Θέλουμε τώρα να υπολογίσουμε τη διασπορά των χαρακτηριστικών του pixel (10,10) για το ψηφίο "0" με βάση τα train δεδομένα. 
Για το σκοπό αυτό ορίζουμε, με ακριβώς αντίστοιχο τρόπο με το Bήμα 4, τη συνάρτηση **digit_variance_at_pixel**. Η μοναδική διαφορά της σε σχέση με την **digit_mean_at_pixel** (του προηγούμενο βήματος) είναι ότι, σαν τελευταίο βήμα, αξιοποιεί τη συνάρτηση **var** της βιβλιοθήκης numpy (αντί της numpy.mean) για τον υπολογισμό της διασποράς μεταξύ των ζητούμενων χαρακτηριστικών.
"""

def digit_variance_at_pixel(X, y, digit, pixel=(10, 10)):
    train_col = pixel[0]*16 + pixel[1]
    train_rows = np.argwhere(y==digit)[:,0]
    pixel_values = [X[train_row,train_col] for train_row in train_rows]
    return np.var(pixel_values)

variance = digit_variance_at_pixel(X_train,y_train,0,(10,10))
print("Variance of pixel (10,10) of digit 0 is:\033[1m",variance)

"""## Βήμα 6

Σε αυτό το βήμα θέλουμε να υπολογίσουμε τη μέση τιμή και διασπορά των χαρακτηριστικών κάθε pixel για το ψηφίο "0" με βάση τα train
δεδομένα. Ακολουθούμε ακριβώς την διαδικασία των βημάτων 4 και 5, ωστόσο τώρα δεν προσδιορίζουμε κάποιο συγκεκριμένο pixel.

Έτσι, ορίζουμε τις συναρτήσεις **digit_mean** και **digit_variance**, σε αντιστοιχία με τις **digit_mean_at_pixel** και **digit_var_at_pixel**. Απλώς τώρα οι νέες συναρτήσεις δέχονται τρία μόνο 
ορίσματα:

*   ένα σύνολο δεδομένων X
*   ένα μονοδιάστατο πίνακα y με τα αντίστοιχα labels του Χ
*   ένα δεδομένο ψηφίο digit 

Μέσω της συνάρτησης **argwhere** της βιβλιοθήκης numpy εντοπίζουμε τις θέσεις του πίνακα y που περιέχουν το ψηφίο digit, και κατά συνέπεια τις γραμμές του πίνακα X που αφορούν το εν λόγω ψηφίο.
Στη συνέχεια, αξιοποιώντας τις συναρτήσεις **mean** και **var** της numpy λαμβάνουμε τη μέση τιμή και διασπορά μεταξύ των γραμμών αυτών. Εδώ δηλαδή δεν εξετάζουμε κάποια συγκεκριμένη στήλη του πίνακα Χ αλλά 
υπολογίζουμε μέση τιμή και διασπορά μεταξύ δειγμάτων του πίνακα Χ που αφορούν ένα δεδομένο ψηφίο, οπότε το τελικό αποτέλεσματα είναι ένας μονοδιάστατος πίνακας 256 χαρακτηριστικών.
"""

def digit_mean(X, y, digit):
    train_rows = np.argwhere(y==digit)[:,0]
    pixels_values = [X[train_row,:] for train_row in train_rows]
    mean_values = np.mean(pixels_values,0)
    return mean_values

def digit_variance(X, y, digit):
    train_rows = np.argwhere(y==digit)[:,0]
    pixels_values = [X[train_row,:] for train_row in train_rows]
    variance_values = np.var(pixels_values,0)
    return variance_values

mean_values = digit_mean(X_train,y_train,0)
variance_values = digit_variance(X_train,y_train,0)

"""## Βήμα 7 και 8

Σχεδιάζουμε το ψηφίο "0" χρησιμοποιώντας τις τιμές της μέσης τιμής και της διασποράς που υπολογίσαμε στο Βήμα 6. Για την απεικόνιση χρησιμοποιούμε την συνάρτηση **imshow** αφού μετασχηματίσουμε πρώτα τα διανύσματα mean_values και variance_values σε πίνακες διαστάσεων 16 x 16.
"""

fig,axs = plt.subplots(1,2,figsize=(10,10))
axs[0].imshow(mean_values.reshape(16,16),cmap='gray')
axs[0].set_title("Mean values for digit 0")
axs[1].imshow(variance_values.reshape(16,16),cmap='gray')
axs[1].set_title("Variance values for digit 0")
plt.show()

"""## Βήμα 9

Υπολογίζουμε τη μέση τιμή και διασπορά των χαρακτηριστικών για όλα τα ψηφία (0-9) με βάση τα train δεδομένα. Συγκεκριμένα, αυτό γίνεται με χρήση των συναρτήσεων **digit_mean** και **digit_variance** ενώ τα αποτελέσματα αποθηκεύονται στους πίνακες means και variances αντίστοιχα, διαστάσεων 10 x 256 ο καθένας. Ταυτόχρονα, αναπαρίστουμε κάθε ψηφίο χρησιμοποιώντας τις 256 τιμές της μέσης τιμής με τρόπο ανάλογο του βήματος 7.
"""

fig, axs = plt.subplots(1,10,figsize=(15,15))
means = np.zeros((10,256))
variances = np.zeros((10,256))
for digit in range(10):
  means[digit,:] = digit_mean(X_train, y_train, digit)
  variances[digit,:] = digit_variance(X_train,y_train,digit)
  axs[digit].imshow(means[digit,:].reshape(16,16),cmap='gray')
  axs[digit].set_title("digit {}".format(digit))

"""## Βήμα 10

Στο βήμα αυτό ταξινομούμε το υπ΄αριθμόν 101 ψηφίο των test δεδομένων (βρίσκεται στη γραμμή 100 του πίνακα X_test αφού η αρίθμηση ξεκινάει από το μηδέν) σε μία από τις 10 κατηγορίες βάσει της Ευκλείδειας απόστασης. Για τον σκοπό αυτό ορίζουμε την συνάρτηση **euclidean_distance** η οποία δέχεται σαν ορίσματα ένα δείγμα 256 διαστάσεων-χαρακτηριστικών και ένα διάνυσμα που αντιπροσωπεύει την μέση τιμή ενός ψηφίου και υπολογίζει την ευκλείδεια απόστασή τους. Αυτό γίνεται με χρήση της συνάρτησης **linalg.norm** της numpy.
"""

def euclidean_distance(s, m):
  return np.linalg.norm(s-m)

"""Για την ταξινόμηση ενός ψηφίου υπολογίζουμε, με χρήση της συνάρτησης **euclidean_distance**, την ευκλείδεια απόστασή του από τα διανύσματα των μέσων τιμών και των 10 ψηφίων/κατηγοριών. Οι αποστάσεις αυτές αποθηκεύονται σειριακά στην λίστα euclidean_distances. Το δείγμα ταξινομείται τελικά στην κλάση εκείνη από της οποίας το μέσο όρο απείχε λιγότερο. Για τον σκοπό αυτό βρίσκουμε την θέση στην οποία η λίστα με τις ευκλείδειες αποστάσεις παίρνει την ελάχιστη τιμή της."""

euclidean_distances = [euclidean_distance(X_test[100],means[digit,:]) for digit in range(10)]
estimated_digit = euclidean_distances.index(min(euclidean_distances))
print("The 101st test sample is estimated to be the \033[1mdigit", estimated_digit)

print(y_test[100])

"""## Βήμα 11

### α)

Ταξινομούμε τώρα όλα τα ψηφία των test δεδομένων σε μία από τις 10 κατηγορίες με βάση την Ευκλείδεια απόσταση. Ορίζουμε την συνάρτηση **euclidean_distance_classifier** η οποία δέχεται σαν ορίσματα έναν πίνακα X με τα δείγματα που θέλουμε να ταξινομήσουμε και τον πίνακα X_mean με τις μέσες τιμές και των 10 ψηφίων. Επιστρέφει με τρόπο ανάλογο του βήματος 10 ένα διάνυσμα διαστάσεων όσο ο αριθμός των δειγμάτων όπου το κάθε στοιχείο του περιέχει την κατηγορία στην οποία ταξινομείται τελικά το αντίστοιχο δείγμα.
"""

def euclidean_distance_classifier(X, X_mean):
    predictions = np.zeros((X.shape[0]))
    for i in range(X.shape[0]):
      euclidean_distances = [euclidean_distance(X[i,:],X_mean[digit,:]) for digit in range(10)]
      estimated_digit = euclidean_distances.index(min(euclidean_distances))
      predictions[i] = estimated_digit
    return predictions

"""### β)

Για τον υπολογισμό του ποσοστού επιτυχίας αρχικοποιούμε μία μεταβλητή την οποία αυξάνουμε κάθε φορά κατά ένα όταν η πρόβλεψη του Ευκλείδειου ταξινομητή συμπίπτει με το label ενός δείγματος (ground truth). Αφού διατρέξουμε όλα τα ψηφία των test δεδομένων το ποσοστό επιτυχίας του ταξινομητή προκύπτει διαιρώντας την τιμή της μεταβλητής αυτής με τον συνολικό αριθμό των δειγμάτων.
"""

correct_est = 0
predictions = euclidean_distance_classifier(X_test, means)
for i in range(len(predictions)):
    if predictions[i]==y_test[i]:
      correct_est += 1
print("Success rate is:\033[1m {} % \033[0m".format((correct_est/len(y_test))*100))

"""## Βήμα 12

Υλοποιούμε τον ταξινομητή ευκλείδειας απόστασης σαν ένα scikit-learn estimator. Συγκεκριμένα ορίζουμε την κλάση **EuclideanDistanceClassifier** η οποία κληρονομεί από τις **BaseEstimator** και **ClassifierMixin**. Ο κατασκευαστής της κλάσης αρχικοποιεί τον πίνακα X_mean_ των μέσων τιμών όλων των ψηφίων (0,1,...,9) και ορίζει τον αριθμό των διαθέσιμων κατηγοριών. Στη συνέχεια υλοποιούμε τις ακόλουθες μεθόδους:



*   **fit**: Δέχεται σαν ορίσματα το train set X μαζί με τα αντίστοιχα labels y και υπολογίζει τον πίνακα X_mean_ των μέσων τιμών όλων των κατηγοριών. Για την εύρεση του διανύσματος της μέσης τιμής μιας κατηγορίας βρίσκουμε αρχικά τις θέσεις όλων των δειγμάτων των οποίων το label ισούται με την εν λόγω κατηγορία. Στη συνέχεια διατρέχουμε τα feature vectors των δειγμάτων αυτών και με χρήση της συνάρτησης **mean** της numpy υπολογίζουμε το νέο διάνυσμα της μέσης τιμής αυτής της κατηγορίας. Το διάνυσμα αυτό αποθηκεύεται στον πίνακα X_mean_. Η διαδικασία αυτή επαναλαμβάνεται για κάθε κατηγορία ώστε να συμπληρωθεί ο πίνακας X_mean_.

*   **predict**: Δέχεται σαν όρισμα το test set X και για κάθε δείγμα σε αυτό εκτιμάει την κατηγορία στην οποία ανήκει επιλέγοντας αυτήν από της οποίας το μέσο όρο απέχει λιγότερο. Η συνάρτηση επιστρέφει ένα διάνυσμα με τις εκτιμήσεις για όλα τα test δεδομένα.  


*   **score**: Δέχεται σαν όρισμα το test set X μαζί με τις αντίστοιχες επισημειώσεις $y$. Καλεί την μέθοδο predict και αναθέτει σε κάθε δείγμα ένα label $\hat{y}$. Για τον υπολογισμό του accuracy score διαιρεί τον αριθμό των σωστών προβλέψεων (εκείνες για τις οποίες το $\hat{y}$ συμπίπτει με το $y$) με τον συνολικό αριθμό των δειγμάτων του συνόλου δοκιμής.
"""

class EuclideanDistanceClassifier(BaseEstimator, ClassifierMixin):  
    """Classify samples based on the distance from the mean feature value"""

    def __init__(self):
        self.X_mean_ = None
        self.y_train_datatype = None
        self.num_of_classes = 10

    def fit(self, X, y):
        self.X_mean_ = np.zeros((self.num_of_classes,X.shape[1]))
        self.y_train_datatype = y.dtype
        for class_dig in range(self.num_of_classes):
            train_rows = np.argwhere(y==class_dig)[:,0]
            pixels_values = [X[train_row,:] for train_row in train_rows]
            self.X_mean_[class_dig,:] = np.mean(pixels_values,0)
        return self

    def predict(self, X):
       predictions = np.zeros(X.shape[0], dtype = self.y_train_datatype)
       for i in range(X.shape[0]):
          euclidean_distances = [np.linalg.norm(X[i]-self.X_mean_[class_dig,:]) for class_dig in range(self.num_of_classes)]
          estimated_digit = euclidean_distances.index(min(euclidean_distances))
          predictions[i] = estimated_digit
       return predictions
       
    def score(self, X, y):
      predictions = self.predict(X) 
      score = (np.argwhere(y==predictions).shape[0])/(y.shape[0])
      return score

model = EuclideanDistanceClassifier()
model.fit(X_train,y_train)
score = model.score(X_test,y_test)
print("Accuracy score is:\033[1m {} % \033[0m".format(score*100))

"""## Βήμα 13

### α)

Στο βήμα αυτό υπολογίζουμε το score του ευκλείδειου ταξινομητή με χρήση 5-fold-cross-validation. Συγκεκριμένα, ορίζουμε τη συνάρτηση **evaluate_classifier** η οποία δέχεται τα ακόλουθα ορίσματα:


*   ένα μοντέλο ταξινόμησης clf
*   ένα σύνολο δεδομένων X : προκύπτει από την συνένωση με χρήση της συνάρτησης **concatenate** της numpy των αρχικών δεδομένων εκπαίδευσης με τα δεδομένα δοκιμής. 
*   τα αντίστοιχα labels y : προκύπτουν από συνένωση των labels των δεδομένων εκπαίδευσης με τα labels των δεδομένων δοκιμής.
*   τον αριθμό των folds : καθορίζει τον αριθμό των διαμερίσεων. 


Τα σύνολα δεδομένων καθώς και τα αντίστοιχα labels που κατασκευάζουμε σε κάθε επανάληψη προκύπτουν με κατάλληλο slicing (για τα test δεδομένα/labels) και με διαγραφή κατάλληλων στοιχείων (για τα train δεδομένα/labels).Το τελευταίο επιτυγχάνεται με χρήση της συνάρτησης **delete** της numpy.
"""

def evaluate_classifier(clf, X, y, folds=5):
    step = X.shape[0]//folds
    i = 0
    crossvalidation_score = 0
    for iteration in range(folds):
        X_test_new = X[i:i+step,:]
        y_test_new = y[i:i+step]
        X_train_new = np.delete(X,slice(i,i+step),0)
        y_train_new = np.delete(y,slice(i,i+step))
        clf.fit(X_train_new,y_train_new)
        crossvalidation_score += clf.score(X_test_new,y_test_new)
        i += step
    return (crossvalidation_score/folds)

model = EuclideanDistanceClassifier()
X_dataset = np.concatenate((X_train,X_test))
y_dataset = np.concatenate((y_train,y_test))
crossvalidation_accuracy = evaluate_classifier(model,X_dataset,y_dataset)
print("\033[1m Our cross validation\033[0m score is:\033[1m {} % \033[0m".format((crossvalidation_accuracy)*100))

"""#### Υλοποίηση του scikit-learn"""

# Calculate accuracy from the sklearn
model = EuclideanDistanceClassifier()
X_dataset = np.concatenate((X_train,X_test))
y_dataset = np.concatenate((y_train,y_test))
scores = cross_validate(model,X_dataset,y_dataset,cv=5)
print("\033[1mScikit-learn 5-fold cross validation \033[0maccuracy is:\033[1m {} % \033[0m".format(np.mean(scores['test_score'])*100))

"""### β)

Χρησιμοποιούμε είναι ο TruncatedSVD για την μείωση των διαστάσεων από 256 σε 50 και ο t-SNE για την τελική μείωση των συνιστωσών σε 2.
"""

X_train_reduced = TruncatedSVD(n_components=50, random_state=0).fit_transform(X_train)
X_train_embedded = TSNE(n_components=2, perplexity=40, verbose=0).fit_transform(X_train_reduced)

"""Για τον σχεδιασμό της περιοχής απόφασης του Ευκλείδειου ταξινομητή εκπαιδεύουμε το μοντέλο μας με τα νέα δισδιάστατα δεδομένα εκπαίδευσης. Επιπλέον, κατασκευάζουμε ένα πλέγμα του οποίου η έκταση καθορίζεται από τις μέγιστες και ελάχιστες τιμές των δεδομένων του train set στις δύο διαστάσεις. Κάθε σημείο αυτού του πλέγματος κατηγοριοποιείται από τον ταξινομητή μας σε μία από τις δέκα διαθέσιμες κατηγορίες. Με βάση τις εκτιμήσεις αυτές και με χρήση της συνάρτησης **contourf** απεικονίζουμε τις περιοχές απόφασης του Ευκλείδειου ταξινομητή."""

min1, max1 = X_train_embedded[:, 0].min()-1, X_train_embedded[:, 0].max()+1
min2, max2 = X_train_embedded[:, 1].min()-1, X_train_embedded[:, 1].max()+1
x1grid = np.arange(min1, max1, 0.05)
x2grid = np.arange(min2, max2, 0.05)
xx, yy = np.meshgrid(x1grid, x2grid)
model = EuclideanDistanceClassifier()
model.fit(X_train_embedded,y_train)
Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

plt.figure(figsize=(40,20))
color_list = ['red','orange','yellow','green','cyan','blue','purple','pink','brown','black'] 
labels = ['0','1','2','3','4','5','6','7','8','9'] 
out = plt.contourf(xx, yy, Z, alpha = 1)
for label in np.unique(y_train):
    plt.scatter(X_train_embedded[y_train==label, 0], X_train_embedded[y_train==label, 1], c=color_list[label], label=label) 
plt.legend(loc='best',fontsize=15)
plt.title("Decision Surface of Euclidean Classifier",fontsize=40)
plt.xlabel("t-SNE component 1",fontsize=30)
plt.ylabel("t-SNE component 2",fontsize=30)
plt.show()

"""### γ)

Για τον σχεδιασμό της καμπύλης εκμάθησης του Ευκλείδειου ταξινομητή χρησιμοποιήσαμε την έτοιμη συνάρτηση **plot_learning_curve** που παρουσιάστηκε [εδώ](https://github.com/georgepar/python-lab/blob/master/Lab%200.3%20Scikit-learn.ipynb) στο εργαστήριο.
"""

from sklearn.model_selection import learning_curve


train_sizes, train_scores, test_scores = learning_curve(
    EuclideanDistanceClassifier(), X_train, y_train, cv=5, n_jobs=-1,train_sizes=np.linspace(0.05, 1.0, 20))

import matplotlib.pyplot as plt 

def plot_learning_curve(train_scores, test_scores, train_sizes, ylim=(0, 1)):
    plt.figure(figsize=(15,8))
    plt.title("Learning Curve for Euclidean Classifier",fontsize=15)
    if ylim is not None:
        plt.ylim(*ylim)
    
    plt.xlabel("Training examples",fontsize=15)
    plt.ylabel("Score",fontsize=15)

    train_scores_mean = np.mean(train_scores, axis=1)
    train_scores_std = np.std(train_scores, axis=1)
    test_scores_mean = np.mean(test_scores, axis=1)
    test_scores_std = np.std(test_scores, axis=1)
    plt.grid()

    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,
                     train_scores_mean + train_scores_std, alpha=0.1,
                     color="r")
    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,
                     test_scores_mean + test_scores_std, alpha=0.1, color="g")
    plt.plot(train_sizes, train_scores_mean, 'o-', color="r",
             label="Training score")
    plt.plot(train_sizes, test_scores_mean, 'o-', color="g",
             label="Cross-validation score")
    plt.legend(loc="best",fontsize=15)
    return plt

plot_learning_curve(train_scores, test_scores, train_sizes, ylim=(0.75, 0.9))
plt.show()

"""## Βήμα 14

Στο βήμα αυτό υπολογίζουμε τις **a-priori** πιθανότητες για κάθε κατηγορία (**class priors**). Για το σκοπό αυτό, αρχικά βρίσκουμε το πλήθος των εμφανίσεων κάθε ψηφίου (δηλαδή κάθε κλάσης) στο σύνολο των train δεδομένων. Αυτό επιτυγχάνεται αξιοποιώντας τη συνάρτηση **count_nonzero** της numpy. Στη συνέχεια, διαιρούμε το πλήθος εμφανίσεων κάθε κατηγορίας με το συνολικό αριθμό δειγμάτων εκπαίδευσης, οπότε και λαμβάνουμε τις ζητούμενες πιθανότητες. Η διαδικασία αυτή υλοποιείται μέσω της συνάρτησης **calculate_priors** που ορίζουμε, η οποία δέχεται σαν ορίσματα ένα σύνολο δεδομένων X καθώς και τα αντίστοιχα labels y ενώ επιστρέφει μια λίστα που περιέχει τις a-priori πιθανότητες όλων των κλάσεων.
"""

def calculate_priors(X, y):
    num_of_train_samples = len(y)
    prior_probabilities = np.zeros(10)
    for digit in range(10):
        prior_probabilities[digit] = (np.count_nonzero(y==digit))/num_of_train_samples
    return prior_probabilities

priors = calculate_priors(X_train,y_train)
print(priors)

"""## Βήμα 15

### α)

Με τρόπο ανάλογο του βήματος 12, υλοποιούμε τον ταξινομητή σε python σαν ένα **scikit-learn estimator**. Συγκεκριμένα, ορίζουμε την κλάση **CustomNBClassifier**, η οποία κληρονομεί από τις **BaseEstimator** και **ClassifierMixin**. Ο κατασκευαστής της κλάσης αρχικοποιεί τους πίνακες X_mean_ και X_var_, των μέσων τιμών και διασπορών όλων των ψηφίων (0-9), καθώς επίσης και τον πίνακα των a-priori πιθανοτήτων κάθε κλάσης, ορίζει τον αριθμό των διαθέσιμων κατηγοριών (10) ενώ τέλος ορίζει και μια boolean μεταβλητή που καθορίζει αν ο πίνακας συνδιακύμανσης κάθε κλάσης θα περιέχει μοναδιαίες διασπορές ή όχι. Στη συνέχεια υλοποιούμε τις ακόλουθες μεθόδους:

*   **fit**: Δέχεται σαν ορίσματα το train set X μαζί με τα αντίστοιχα labels y και υπολογίζει τους πίνακες X_mean_ και X_var_ των μέσων τιμών και διασπορών όλων των κατηγοριών. Για την εύρεση των διανυσμάτων μέσης τιμής και διασποράς μιας κατηγορίας, αρχικά εντοπίζουμε τις θέσεις όλων των δειγμάτων των οποίων το label ισούται με την εν λόγω κατηγορία. Στη συνέχεια, διατρέχουμε τα feature vectors των δειγμάτων αυτών και με χρήση των συναρτήσεων **mean** και **var** της numpy υπολογίζουμε τα  διανύσματα μέσης τιμής και διασποράς αυτής της κατηγορίας. Έπειτα τα αποθηκεύουμε στους πίνακες X_mean_ και X_var_ αντίστοιχα. Επαναλαμβάνουμε τη διαδικασία για κάθε κατηγορία. Προς αποφυγή του μηδενισμού της ορίζουσας του πίνακα συνδιακύμνασης κάθε κλάσης, προσθέτουμε μια μικρή σταθερά $(10^{-5})$ στις μηδενικές τιμές του πίνακα. Να σημειωθεί πως σε περίπτωση που η boolean μεταβλητή use_unit_variance δοθεί ως True τότε αντί για τον υπολογισμό της διασποράς κάθε κλάσης με την προαναφερθείσα διαδικασία, θέτουμε απλώς τη διασπορά των χαρακτηριστικών κάθε κλάσης ίση με τη μονάδα.

*   **predict**: Δέχεται σαν όρισμα ένα σύνολο ελέγχου X και για κάθε δείγμα του υπολογίζει τις πιθανότητες αυτό να ανήκει σε μία από τις 10 κατηγορίες (0-9). Οι πιθανότητες αυτές αποθηκεύονται στον δισδιάστατο πίνακα criterion, μεγέθους 10 $\times$ test samples. Με χρήση της συνάρτησης argmax της numpy κρατάμε τη μέγιστη τιμή που αντιστοιχεί σε κάθε στήλη του πίνακα (πιθανότερη κατηγορία κάθε sample) οπότε τελικά η συνάρτηση επιστρέφει ένα μονοδιάστατο πίνακα predictions με τις προβλέψεις για κάθε test δείγμα.

*   **score**: Δέχεται σαν όρισμα το test set X μαζί με τις αντίστοιχες επισημειώσεις $y$. Καλεί την μέθοδο predict και αναθέτει σε κάθε δείγμα ένα label $\hat{y}$. Για τον υπολογισμό του accuracy score διαιρεί τον αριθμό των σωστών προβλέψεων (εκείνες για τις οποίες το $\hat{y}$ συμπίπτει με το $y$) με τον συνολικό αριθμό των δειγμάτων του συνόλου δοκιμής.
"""

class CustomNBClassifier(BaseEstimator, ClassifierMixin):
    """Custom implementation Naive Bayes classifier"""

    def __init__(self, use_unit_variance=False):
        self.X_mean_ = None
        self.X_var_ = None
        self.apriori = None
        self.y_train_datatype = None
        self.num_of_classes = 10
        self.use_unit_variance = use_unit_variance


    def fit(self, X, y):
        self.X_mean_ = np.zeros((self.num_of_classes,X.shape[1]))
        self.X_var_ = np.zeros((self.num_of_classes,X.shape[1])) # Σ
        self.apriori = np.zeros(self.num_of_classes)
        self.y_train_datatype = y.dtype
        for class_dig in range(self.num_of_classes):
            train_rows = np.argwhere(y==class_dig)[:,0]
            pixels_values = [X[train_row,:] for train_row in train_rows]
            self.apriori[class_dig] = (np.count_nonzero(y==class_dig))/len(y)
            self.X_mean_[class_dig,:] = np.mean(pixels_values,0)
            if not self.use_unit_variance:
                  self.X_var_[class_dig,:] = np.var(pixels_values,0)
            else: continue
        if self.use_unit_variance: self.X_var_ = np.ones((self.num_of_classes,X.shape[1]))
        else: self.X_var_[self.X_var_==0] = 1e-5
        return self

    def predict(self, X):
        predictions = np.zeros(X.shape[0])
        criterion = np.zeros((self.num_of_classes,X.shape[0]))
        for class_dig in range(self.num_of_classes):
            ln_det_Sigma = sum(np.log(self.X_var_[class_dig,:]))
            inv_Sigma = np.linalg.inv(np.diag(self.X_var_[class_dig,:]))            
            for i in range(X.shape[0]):
                difference  = X[i,:] - self.X_mean_[class_dig,:]
                criterion[class_dig,i] = np.log(self.apriori[class_dig]) -  (X.shape[1]/2) * np.log(2*np.pi) - (1/2)*ln_det_Sigma - (1/2)*(np.transpose(difference) @ inv_Sigma @ difference)
        predictions = np.argmax(criterion,axis=0).astype(self.y_train_datatype)
        return predictions

    def score(self, X, y):
        predictions = self.predict(X).astype(self.y_train_datatype)
        score = (np.argwhere(y==predictions).shape[0])/(y.shape[0])
        return score

"""### β)

Για τον υπολογισμό του σκορ του Naive Bayes ταξινομητή που κατασκευάσαμε, ορίζουμε τη συνάρτηση **evaluate_custom_nb_classifier** η οποία δέχεται ως ορίσματα ένα σύνολο δεδομένων X, τα αντίστοιχα labels y καθώς και μια παράμετρο folds (με default τιμή 5) που καθορίζει τον αριθμό των διαμερίσεων για το cross validation. Η συνάρτηση ορίζει ένα μοντέλο **CustomNBClassifier** και έπειτα υπολογίζει το score του αξιοποιώντας τη συνάρτηση **evaluate_classifier** που ορίσαμε στο βήμα 13 (α).
"""

def evaluate_custom_nb_classifier(X, y, folds=5):
    customNB = CustomNBClassifier()
    crossvalidation_score = evaluate_classifier(customNB,X,y,folds)
    return crossvalidation_score

print("\033[1m Custom Gaussian Naive Bayes Classifier \033[0mscore is:\033[1m {} % \033[0m".format(evaluate_custom_nb_classifier(X_dataset, y_dataset, folds=5)*100))

"""### γ)

Για τον υπολογισμό του σκορ του ταξινομητή του **scikit-learn** ορίζουμε τη συνάρτηση **evaluate_sklearn_nb_classifier**, η οποία δέχεται ακριβώς τα ίδια ορίσματα με τη συνάρτηση **evaluate_custom_nb_classifier** που ορίσαμε προηγουμένως. Η συνάρτηση ορίζει ένα μοντέλο **GaussianNB** και έπειτα υπολογίζει το score του, αξιοποιώντας τη συνάρτηση **evaluate_classifier** που ορίσαμε στο βήμα 13 (α).
"""

def evaluate_sklearn_nb_classifier(X, y, folds=5):
    sklearnNB = GaussianNB()
    crossvalidation_score = evaluate_classifier(sklearnNB,X,y,folds)
    return crossvalidation_score

print("\033[1m Sklearn Gaussian Naive Bayes Classifier \033[0mscore is:\033[1m {} % \033[0m".format(evaluate_sklearn_nb_classifier(X_dataset, y_dataset, folds=5)*100))

"""#### Τροποποίηση CustomNBClassifier

Για λόγους σύγκρισης, τροποποιούμε την κλάση CustomNBClassifier και δημιουργούμε μια νέα, την SmoothCustomNBClassifier, η οποία δέχεται μία επιπλέον παράμετρο, **var_smoothing**, ώστε να ταυτιστεί η υλοποίηση μας με αυτήν του scikit-learn.
"""

class SmoothCustomNBClassifier(BaseEstimator, ClassifierMixin):
    """Custom implementation Naive Bayes classifier"""

    def __init__(self, var_smoothing = 1e-9, use_unit_variance=False):
        self.X_mean_ = None
        self.X_var_ = None
        self.apriori = None
        self.y_train_datatype = None
        self.num_of_classes = 10
        self.var_smoothing = var_smoothing
        self.use_unit_variance = use_unit_variance


    def fit(self, X, y):
        self.X_mean_ = np.zeros((self.num_of_classes,X.shape[1]))
        self.X_var_ = np.zeros((self.num_of_classes,X.shape[1])) # Σ
        self.apriori = np.zeros(self.num_of_classes)
        self.y_train_datatype = y.dtype
        for class_dig in range(self.num_of_classes):
            train_rows = np.argwhere(y==class_dig)[:,0]
            pixels_values = [X[train_row,:] for train_row in train_rows]
            self.apriori[class_dig] = (np.count_nonzero(y==class_dig))/len(y)
            self.X_mean_[class_dig,:] = np.mean(pixels_values,0)
            if not self.use_unit_variance:
                  self.X_var_[class_dig,:] = np.var(pixels_values,0)
            else: continue
        if self.use_unit_variance: self.X_var_ = np.ones((self.num_of_classes,X.shape[1]))
        else: self.X_var_= self.X_var_ + self.var_smoothing*np.max(self.X_var_) # get rid of zeros
        return self

    def predict(self, X):
        predictions = np.zeros(X.shape[0])
        criterion = np.zeros((self.num_of_classes,X.shape[0]))
        for class_dig in range(self.num_of_classes):
            ln_det_Sigma = sum(np.log(self.X_var_[class_dig,:]))
            inv_Sigma = np.linalg.inv(np.diag(self.X_var_[class_dig,:]))            
            for i in range(X.shape[0]):
                difference  = X[i,:] - self.X_mean_[class_dig,:]
                criterion[class_dig,i] = np.log(self.apriori[class_dig]) -  (X.shape[1]/2) * np.log(2*np.pi) - (1/2)*ln_det_Sigma - (1/2)*(np.transpose(difference) @ inv_Sigma @ difference)
        predictions = np.argmax(criterion,axis=0).astype(self.y_train_datatype)
        return predictions

    def score(self, X, y):
        predictions = self.predict(X).astype(self.y_train_datatype)
        score = (np.argwhere(y==predictions).shape[0])/(y.shape[0])
        return score

"""#### Σύγκριση με την υλοποίηση του scikit learn"""

def evaluate_smooth_custom_nb_classifier(X, y, folds=5):
    smoothcustomNB = SmoothCustomNBClassifier(var_smoothing=1e-6)
    crossvalidation_score = evaluate_classifier(smoothcustomNB,X,y,folds)
    return crossvalidation_score

print("\033[1m Custom Gaussian Naive Bayes Classifier \033[0mscore is:\033[1m {} % \033[0m".format(evaluate_smooth_custom_nb_classifier(X_dataset, y_dataset, folds=5)*100))

sklearnNB = GaussianNB(var_smoothing=1e-6)
print("\033[1m Custom Gaussian Naive Bayes Classifier \033[0mscore is:\033[1m {} % \033[0m".format(evaluate_classifier(sklearnNB,X_dataset,y_dataset,folds=5)*100))

scores = []
x = [10**i for i in range(-10,-1)]
for i in x:
    sklearnNB = GaussianNB(var_smoothing=i)
    crossvalidation_score = evaluate_classifier(sklearnNB,X_dataset,y_dataset,folds=5)
    scores.append(crossvalidation_score)

plt.figure(figsize=(10,5))
plt.plot(x,scores)
plt.xlabel("variance smoothing factor",fontsize = 10)
plt.ylabel("GaussianNB accuracy",fontsize = 10)
plt.xscale('log')

"""## Βήμα 16

Ορίζουμε ξανά ένα μοντέλο CustomNBClassifier, μόνο που τώρα θέτουμε την τιμή unit_variance σε True (αντί για False που ήταν στο βήμα 15). Καλούμε λοιπόν τη συνάρτηση evaluate_classifier με ορίσματα: το μοντέλο, τα σύνολα X_dataset, y_dataset και τον αριθμό των folds (5).
"""

customNB = CustomNBClassifier(use_unit_variance=True)
crossvalidation_score = evaluate_classifier(customNB,X_dataset,y_dataset,folds=5)
print("\033[1m Naive Bayes Classifier \033[0mscore is:\033[1m {} % \033[0m".format(crossvalidation_score*100))

"""## Βήμα 17

Συγκρίνουμε τώρα την επίδοση των ταξινομητών Naive Bayes, Nearest Neighbors, SVM (με διαφορετικούς kernels) και EuclideanDistance. Για τον σκοπό αυτό δημιουργούμε τις συναρτήσεις **evaluate_knn_classifier**, **evaluate_linear_svm_classifier**, **evaluate_rbf_svm_classifier** και **evaluate_euclidean_classifier**, οι οποίες ορίζουν το αντίστοιχο μοντέλο χρησιμοποιώντας τις έτοιμες υλοποιήσεις του scikit-learn, και υπολογίζουν το 5-fold cross-validation score καλώντας την **evaluate_classifier** του βήματος 13 (α).

### KNeighbors
"""

def evaluate_knn_classifier(X, y, folds=5):
    knn = KNeighborsClassifier(n_neighbors=3)
    crossvalidation_score = evaluate_classifier(knn,X,y,folds)
    return crossvalidation_score

print("\033[1m KNeighbors Classifier \033[0mscore is:\033[1m {} % \033[0m".format(evaluate_knn_classifier(X_dataset, y_dataset, folds=5)*100))

"""### Linear SVM"""

def evaluate_linear_svm_classifier(X, y, folds=5):
    linearSVM = SVC(kernel='linear',gamma='scale',probability=True)
    crossvalidation_score = evaluate_classifier(linearSVM,X,y,folds)
    return crossvalidation_score

print("\033[1m Linear SVM Classifier \033[0mscore is:\033[1m {} % \033[0m".format(evaluate_linear_svm_classifier(X_dataset, y_dataset, folds=5)*100))

"""### Rbf SVM"""

def evaluate_rbf_svm_classifier(X, y, folds=5):
    rbfSVM = SVC(kernel='rbf',gamma='scale',probability=True)
    crossvalidation_score = evaluate_classifier(rbfSVM,X,y,folds)
    return crossvalidation_score

print("\033[1m Rbf SVM Classifier \033[0mscore is:\033[1m {} % \033[0m".format(evaluate_rbf_svm_classifier(X_dataset, y_dataset, folds=5)*100))

"""### Euclidean"""

def evaluate_euclidean_classifier(X, y, folds=5):
    euclidean = EuclideanDistanceClassifier()
    crossvalidation_score = evaluate_classifier(euclidean,X,y,folds)
    return crossvalidation_score

print("\033[1m Euclidean Classifier \033[0mscore is:\033[1m {} % \033[0m".format(evaluate_euclidean_classifier(X_dataset, y_dataset, folds=5)*100))

"""### Confusion Matrix

Με χρήση της συνάρτησης **plot_confusion_matrix** απεικονίζουμε τους πίνακες σύγχυσης των παραπάνω ταξινομητών.
"""

fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(20,12))
class_names = np.array(['0','1','2','3','4','5','6','7','8','9'])
classifiers = [CustomNBClassifier(), CustomNBClassifier(use_unit_variance=True), GaussianNB(), KNeighborsClassifier(n_neighbors=3), SVC(kernel='linear',gamma='scale',probability=True), SVC(kernel='rbf',gamma='scale',probability=True)]
classifiers_names = ['Custom Gaussian NB', 'Custom Gaussian NB with unit variances', 'Scikit-learn Gaussian NB', 'Scikit-learn 3-Nearest Neighbors', 'Scikit-learn Linear SVM', 'Scikit-learn Rbf SVM' ]
cnt = 0
for classifier, ax in zip(classifiers, axes.flatten()):
    fitted_classifier = classifier.fit(X_train,y_train)
    plot_confusion_matrix(fitted_classifier, 
                          X_test, 
                          y_test, 
                          ax=ax, 
                          cmap=plt.cm.Blues,
                          display_labels=class_names,
                          values_format='')
    ax.title.set_text(classifiers_names[cnt])
    cnt += 1

plt.tight_layout()  
plt.show()

"""### Βήμα 18

### α)

Ορίζουμε την συνάρτηση **evaluate_voting_classifier** η οποία δέχεται για ορίσματα ένα dataset X μαζί με τα αντίστοιχα labels y και τον αριθμό των folds. Αφού ορίσει τους επιμέρους ταξινομητές καλεί την **VotingClassifier** για τον συνδυασμό τους. Ο υπολογισμός του 5-fold cross-validation-score γίνεται με χρήση της συνάρτησης **evaluate_classifier** του βήματος 13 (α).

#### Hard voting
"""

def evaluate_voting_classifier(X, y, folds=5):
    rbfSVM = SVC(kernel='rbf',gamma='scale',probability=True)
    nn3 = KNeighborsClassifier(n_neighbors=3)
    nb =  GaussianNB()
    eclf = VotingClassifier(estimators=[('rbfsvm',rbfSVM), ('nn',nn3), ('nb',nb)], voting='hard')
    crossvalidation_score = evaluate_classifier(eclf,X,y,folds)
    return crossvalidation_score

print("\033[1m Voting Classifier \033[0mscore with \033[1mhard voting \033[0mis:\033[1m {} % \033[0m".format(evaluate_voting_classifier(X_dataset, y_dataset, folds=5)*100))

"""#### Soft voting"""

def evaluate_voting_classifier(X, y, folds=5):
    rbfSVM = SVC(kernel='rbf',gamma='scale',probability=True)
    nn3 = KNeighborsClassifier(n_neighbors=3)
    nb = GaussianNB()
    eclf = VotingClassifier(estimators=[('rbfsvm',rbfSVM), ('nn',nn3), ('nb',nb)], voting='soft')
    crossvalidation_score = evaluate_classifier(eclf,X,y,folds)
    return crossvalidation_score

print("\033[1m Voting Classifier \033[0mscore with \033[1msoft voting \033[0mis:\033[1m {} % \033[0m".format(evaluate_voting_classifier(X_dataset, y_dataset, folds=5)*100))

"""### β)

Για την υλοποίηση του **BaggingClassifier** χρησιμοποιούμε την ομώνυμη συνάρτηση της scikit-learn ενώ ο ταξινομητής που επιλέγουμε από τα προηγούμενα βήματα είναι ο Rbf SVM που είχε την μεγαλύτερη απόδοση.
"""

def evaluate_bagging_classifier(X, y, folds=5):
    rbfSVM = SVC(kernel='rbf',gamma='scale',probability=True)
    bag = BaggingClassifier(rbfSVM,max_samples=0.5)
    crossvalidation_score = evaluate_classifier(bag,X,y,folds)
    return crossvalidation_score

print("\033[1m Bagging Classifier \033[0mscore is:\033[1m {} % \033[0m".format(evaluate_bagging_classifier(X_dataset, y_dataset, folds=5)*100))

"""## Βήμα 19"""

from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, utils
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torch
from torch.utils.data import random_split
import tensorflow as tf

"""### α)"""

class DigitDataset(Dataset):
    
    def __init__(self,X,y,trans=None):
        self.X_train_ = torch.from_numpy(X).type(torch.FloatTensor) 
        self.y_train_ = torch.from_numpy(y).type(torch.FloatTensor) 
        self.trans = trans 

    def __len__(self):
        return self.X_train_.shape[0]
    
    def __getitem__(self,index):
        
        if self.trans is not None: 
            X_train_[index] = self.trans(self.X_train_[index])
            y_train_[index] = self.trans(self.y_train_[index])
        
        return self.X_train_[index],self.y_train_[index]

class DigitDataset_(Dataset):
    
    def __init__(self,X,trans=None):
        self.X_train_ = torch.from_numpy(X).type(torch.FloatTensor) 
        self.trans = trans 

    def __len__(self):
        return self.X_train_.shape[0]
    
    def __getitem__(self,index):
        
        if self.trans is not None: 
            X_train_[index] = self.trans(self.X_train_[index])
        
        return self.X_train_[index]

"""### β)"""

class Fully_Connected_NN(nn.Module):
    
    def __init__(self, D_in, H, D_out):

        super(Fully_Connected_NN,self).__init__()
        self.input_layer  = nn.Linear(D_in,H)
        self.hidden_layer = nn.Linear(H,H)
        self.output_layer = nn.Linear(H,D_out)
                
    def forward(self,x):

        x = self.input_layer(x)
        x = F.relu(self.hidden_layer(x))
        x =  self.output_layer(x)
        return x

class PytorchNNModel(BaseEstimator, ClassifierMixin):
    def __init__(self, model, criterion, optimizer ,number_of_epochs, batch_size):
        # WARNING: Make sure predict returns the expected (nsamples) numpy array not a torch tensor.
        self.model = model
        self.criterion = criterion 
        self.optimizer = optimizer
        self.n_epochs = number_of_epochs
        self.batch_size = batch_size

    def dataloader(self, dataset, batch_size = 32):
        data_in_batches = []
        for i in range(0,dataset.__len__()-batch_size, batch_size):
            data_batch = dataset[i:i+batch_size][0]
            data_labels = dataset[i:i+batch_size][1]
            data_in_batches.append((data_batch,data_labels))

       # Create the last data batch with the remaining samples
        data_batch = dataset[i+batch_size:][0]
        data_labels = dataset[i+batch_size:][1] 
        data_in_batches.append((data_batch,data_labels))
        return data_in_batches

    def dataloader_(self, dataset, batch_size = 32):
        data_in_batches = []
        for i in range(0,dataset.__len__()-batch_size, batch_size):
            data_batch = dataset[i:i+batch_size]
            data_in_batches.append(data_batch)

       # Create the last data batch with the remaining samples
        data_batch = dataset[i+batch_size:]
        data_in_batches.append(data_batch)
        return data_in_batches

    def fit(self, X, y):
        train_dataset = DigitDataset(X,y)
        # Split train dataset
        train_size = int(0.8 * X.shape[0])
        valid_size = X.shape[0] - train_size
        partial_train_dataset , valid_dataset = random_split(train_dataset, [train_size, valid_size])
        train_loader = self.dataloader(partial_train_dataset)
        val_loader = self.dataloader(valid_dataset)
        for epoch in range(self.n_epochs):
            self.model.train()
            train_loss = 0
            for data, target in train_loader:
                self.optimizer.zero_grad()
                output = self.model(data)
                loss = self.criterion(output,target.long())
                loss.backward()
                optimizer.step()
                train_loss += loss.item()
            self.model.eval()
            valid_loss = 0
            correct = 0 
            n_samples = 0
            with torch.no_grad():
                for data, target in val_loader:
                    output = self.model(data)
                    loss = self.criterion(output, target.long()) 
                    valid_loss += loss.item()
                    predicted = torch.argmax(output, 1)  
                    correct += (predicted == target).sum().item() 
                    n_samples += len(target)
            train_loss /= len(train_loader)
            valid_loss /= len(val_loader)
            valid_accuracy = correct / n_samples 
            if not epoch%20: print(f'Epoch: {epoch+1}/{self.n_epochs}.. Training loss: {train_loss}.. Validation Loss: {valid_loss}.. Validation Accuracy: {valid_accuracy}')
        return self
        
    def predict(self, X):
        # TODO: wrap X in a test loader and evaluate
        predictions = np.zeros(self.batch_size)
        test_dataset = DigitDataset_(X)
        test_loader = self.dataloader_(test_dataset)
        self.model.eval()
        with torch.no_grad():
            for data in test_loader:
                output = self.model(data)
                predicted = torch.argmax(output,1)
                predicted_ = predicted.numpy()
                predictions = np.concatenate([predictions,predicted_])
        return predictions[self.batch_size:]

    def score(self, X, y):
        # Return accuracy score.
        predictions = self.predict(X)
        score = (np.argwhere(y==predictions).shape[0])/(y.shape[0])
        return score

model = Fully_Connected_NN(256,256,10)
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=0.003)

pytorch_model = PytorchNNModel(model,criterion,optimizer,200,32)
pytorch_model.fit(X_train,y_train)
print("\033[1m Neural Network \033[0mscore is:\033[1m {} % \033[0m".format(pytorch_model.score(X_test,y_test)*100))

def evaluate_nn_classifier(X, y, folds=5):
    model = Fully_Connected_NN(256,256,10)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(model.parameters(), lr=0.003)
    pytorch_model = PytorchNNModel(model,criterion,optimizer,200,32)
    crossvalidation_score = evaluate_classifier(pytorch_model,X,y,folds)
    return crossvalidation_score