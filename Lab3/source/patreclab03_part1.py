# -*- coding: utf-8 -*-
"""PatRecLab03_Part1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1F_Eyal-ZI5qHRp4ibhzrwQ-uQxcovHk4
"""

# To add the competition data Click File->Add or Upload data-> Search by URL -> https://www.kaggle.com/geoparslp/patreco3-multitask-affective-music

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the "../input/" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# Any results you write to the current directory are saved as output.

# Helper functions to read fused, mel, and chromagram
def read_fused_spectrogram(spectrogram_file):
    spectrogram = np.load(spectrogram_file)
    return spectrogram.T


def read_mel_spectrogram(spectrogram_file):
    spectrogram = np.load(spectrogram_file)[:128]
    return spectrogram.T

    
def read_chromagram(spectrogram_file):
    spectrogram = np.load(spectrogram_file)[128:]
    return spectrogram.T

# Plot the spectrogram and the chromagram

import librosa.display
import matplotlib.pyplot as plt

def plot_spectrogram(mel1,label1,mel2,label2):

    plt.subplots(1,2,figsize=(18,7))  
    plt.subplot(1,2,1)
    librosa.display.specshow(mel1,x_axis='time',y_axis='linear')
    plt.title("Mel spectrogram of \"{}\" category".format(label1),fontsize=14)
    plt.subplot(1,2,2)
    librosa.display.specshow(mel2,x_axis='time',y_axis='linear')
    plt.title("Mel spectrogram of \"{}\" category".format(label2),fontsize=14)
    plt.show()
    
def plot_chromagram(chroma1,label1,chroma2,label2):
    plt.subplots(1,2,figsize=(18,7))  
    plt.subplot(1,2,1)
    librosa.display.specshow(chroma1, y_axis='chroma', x_axis='time')
    plt.title("Chromagram of \"{}\" category".format(label1),fontsize=14)
    plt.subplot(1,2,2)
    librosa.display.specshow(chroma2, y_axis='chroma', x_axis='time')
    plt.title("Chromagram of \"{}\" category".format(label2),fontsize=14)
    plt.show()

"""### Βήμα 1: Εξοικείωση με φασματογραφήματα στην κλίμακα mel"""

path = "/kaggle/input/patreco3-multitask-affective-music/data/fma_genre_spectrograms/"  # Path for Mel Spectrogram Dataset

f = open(path +"/train_labels.txt","r")                                                 # Open txt file
lines = f.readlines()                                                                   # Read the lines of the file  
num_lines = len(lines)                                                                  # Keep the number of lines
 
first_index = np.random.choice(range(1,num_lines), 1)[0]                                # Choose randomly a line       
first_id = lines[first_index].split()[0].split('.')[0]+".fused.full.npy"                # Define the spectrogram id
first_label = lines[first_index].split()[1]                                             # Define the music type (label)

while True:                                                                             # Repeat the following process
    second_index = np.random.choice(range(1,num_lines), 1)[0]                           # Choose randomly a line    
    second_id = lines[second_index].split()[0].split('.')[0]+".fused.full.npy"          # Define the spectrogram id
    second_label = lines[second_index].split()[1]                                       # Define the Genre (label)
    if second_label!=first_label:                                                       # If the genre is the same as the previous one then choose another line
        break                                                                           # If the genre is different than the previous one then keep it

print("First random selection is file \"\033[1m{}\033[0m\" that belongs to \"\033[1m{}\033[0m\" category".format(first_id,first_label))        # Print 1st spectrogram Id and Genre
print("Second random selection is file \"\033[1m{}\033[0m\" that belongs to \"\033[1m{}\033[0m\" category\n".format(second_id,second_label))   # Print 2nd spectrogram Id and Genre

    
spec1, spec2 = read_fused_spectrogram(path+"train/"+ first_id).T, read_fused_spectrogram(path+"train/"+ second_id).T   # Read the 2 fused spectrograms
mel1, mel2 = read_mel_spectrogram(path+"train/"+ first_id).T, read_mel_spectrogram(path+"train/"+ second_id).T         # Read the 2 Mel spectrogeams

plot_spectrogram(mel1,first_label,mel2,second_label)                                                                   # Plot the 2 Mel spectrograms

"""### Βήμα 2: Συγχρονισμός φασματογραφημάτων στο ρυθμό της μουσικής (beat-synced spectrograms)

#### (a)
"""

print("First spectrogram's shape is: {}".format(spec1.shape))                   # Print first spectrogram's shape 
print("First mel spectrogram's shape is: {}\n".format(mel1.shape))              # Print first mel spectrogram's shape
print("Second spectrogram's shape is: {}".format(spec2.shape))                  # Print second spectrogram's shape
print("Second mel spectrogram's shape is: {}".format(mel2.shape))               # Print second mel spectrogram's shape

"""#### (b)"""

path_beat = "/kaggle/input/patreco3-multitask-affective-music/data/fma_genre_spectrograms_beat/"                                            # Path for Beat Synced Spectrogram Dataset

spec1_beat, spec2_beat = read_fused_spectrogram(path_beat+"train/"+ first_id).T, read_fused_spectrogram(path_beat+"train/"+ second_id).T    # Read the 2 beat synced fused spectrograms
mel1_beat, mel2_beat = read_mel_spectrogram(path_beat+"train/"+ first_id).T, read_mel_spectrogram(path_beat+"train/"+ second_id).T          # Read the 2 beat synced mel spectrograms

# Print their shapes
print("First beat-synced spectrogram's shape is: {}".format(spec1_beat.shape))                                                              
print("First beat-synced mel-spectrogram's  shape is: {}\n".format(mel1_beat.shape))                                                        

print("Second beat-synced spectrogram's shape is: {}".format(spec2_beat.shape))
print("Second beat-synced mel-spectrogram's  shape is: {}".format(mel2_beat.shape))
    
plot_spectrogram(mel1_beat,first_label,mel2_beat,second_label)                                                                               # Plot the 2 beat synced mel spectrograms

"""### Βήμα 3: Εξοικείωση με χρωμογραφήματα"""

chroma1, chroma2 = read_chromagram(path+"train/"+ first_id).T, read_chromagram(path+"train/"+ second_id).T        # Read the 2 chromagrams

# Print their shapes
print("First chromagram's shape is: {}".format(chroma1.shape))
print("Second chromagram's shape is: {}".format(chroma2.shape))

plot_chromagram(chroma1,first_label,chroma2,second_label)                                                         # Plot the 2 chromagrams

chroma1_beat, chroma2_beat = read_chromagram(path_beat+"train/"+ first_id).T, read_chromagram(path_beat+"train/"+ second_id).T      # Read the 2 beat synced chromagrams

# Print their shapes
print("First beat-synced chromagram's shape is: {}".format(chroma1_beat.shape))
print("Second beat-synced chromagram's shape is: {}".format(chroma2_beat.shape))

plot_chromagram(chroma1_beat,first_label,chroma2_beat,second_label)                                                                 # Plot the 2 chromagrams

"""### Βήμα 4: Φόρτωση και ανάλυση δεδομένων

#### (a)
"""

# Combine similar classes and remove underrepresented classes
class_mapping = {
    'Rock': 'Rock',
    'Psych-Rock': 'Rock',
    'Indie-Rock': None,
    'Post-Rock': 'Rock',
    'Psych-Folk': 'Folk',
    'Folk': 'Folk',
    'Metal': 'Metal',
    'Punk': 'Metal',
    'Post-Punk': None,
    'Trip-Hop': 'Trip-Hop',
    'Pop': 'Pop',
    'Electronic': 'Electronic',
    'Hip-Hop': 'Hip-Hop',
    'Classical': 'Classical',
    'Blues': 'Blues',
    'Chiptune': 'Electronic',
    'Jazz': 'Jazz',
    'Soundtrack': None,
    'International': None,
    'Old-Time': None
}

def get_files_labels(txt, class_mapping=None):
    with open(txt, 'r') as fd:
        lines = [l.rstrip().split('\t') for l in fd.readlines()[1:]]
        files, labels = [], []
        for l in lines:
            label = l[1]
            if class_mapping:
                label = class_mapping[l[1]]
            if not label:
                continue
            # Kaggle automatically unzips the npy.gz format so this hack is needed
            _id = l[0].split('.')[0]
            npy_file = '{}.fused.full.npy'.format(_id)
            files.append(npy_file)
            labels.append(label)
        return files, labels

import copy
from sklearn.preprocessing import LabelEncoder
from torch.utils.data import Dataset
from torch.utils.data import SubsetRandomSampler, DataLoader
import re

# TODO: Comment on how the train and validation splits are created.
# TODO: It's useful to set the seed when debugging but when experimenting ALWAYS set seed=None. Why?
def torch_train_val_split(
        dataset, batch_train, batch_eval,
        val_size=.2, shuffle=True, seed=None):
    # Creating data indices for training and validation splits:
    dataset_size = len(dataset)
    indices = list(range(dataset_size))
    val_split = int(np.floor(val_size * dataset_size))
    if shuffle:
        np.random.seed(seed)
        np.random.shuffle(indices)
    train_indices = indices[val_split:]
    val_indices = indices[:val_split]

    # Creating PT data samplers and loaders:
    train_sampler = SubsetRandomSampler(train_indices)
    val_sampler = SubsetRandomSampler(val_indices)

    train_loader = DataLoader(dataset,
                              batch_size=batch_train,
                              sampler=train_sampler)
    val_loader = DataLoader(dataset,
                            batch_size=batch_eval,
                            sampler=val_sampler)
    return train_loader, val_loader


class LabelTransformer(LabelEncoder):
    def inverse(self, y):
        try:
            return super(LabelTransformer, self).inverse_transform(y)
        except:
            return super(LabelTransformer, self).inverse_transform([y])

    def transform(self, y):
        try:
            return super(LabelTransformer, self).transform(y)
        except:
            return super(LabelTransformer, self).transform([y])


# TODO: Comment on why padding is needed
class PaddingTransform(object):
    def __init__(self, max_length, padding_value=0):
        self.max_length = max_length
        self.padding_value = padding_value

    def __call__(self, s):
        if len(s) == self.max_length:
            return s

        if len(s) > self.max_length:
            return s[:self.max_length]
        
        if len(s) < self.max_length:
            s1 = copy.deepcopy(s)
            pad = np.zeros((self.max_length - s.shape[0], s.shape[1]), dtype=np.float32)
            s1 = np.vstack((s1, pad))
            return s1

# Pytorch Dataset Class for creating the dataset
class SpectrogramDataset(Dataset):
    def __init__(self, path, class_mapping=None, train=True, max_length=-1, read_spec_fn=read_fused_spectrogram):
        t = 'train' if train else 'test'
        p = os.path.join(path, t)
        self.index = os.path.join(path, "{}_labels.txt".format(t))
        self.files, labels = get_files_labels(self.index, class_mapping)
        self.feats = [read_spec_fn(os.path.join(p, f)) for f in self.files] # lista apo arrays opoy to kathe ena einai to mel / chromogram
        self.feat_dim = self.feats[0].shape[1]
        self.lengths = [len(i) for i in self.feats]
        self.max_length = max(self.lengths) if max_length <= 0 else max_length
        self.zero_pad_and_stack = PaddingTransform(self.max_length)
        self.label_transformer = LabelTransformer()
        if isinstance(labels, (list, tuple)):
            self.labels = np.array(self.label_transformer.fit_transform(labels)).astype('int64')
    def __getitem__(self, item):
        # TODO: Inspect output and comment on how the output is formatted
        l = min(self.lengths[item], self.max_length)
        return self.zero_pad_and_stack(self.feats[item]), self.labels[item], l

    def __len__(self):
        return len(self.labels)

"""#### (c)"""

def plot_histogram(keys,values,colors):
    plt.figure(figsize=(18,7))                       # Define figure size
    plt.bar(keys,values,width=0.7,color=colors)      # Plot values
    plt.title("Histogram of classes",fontsize=15)    # Title
    plt.xticks(rotation=45)                          # Rotate labels 45 degrees
    plt.margins(x=0.01)                              # Νarrow margins
    plt.show()

_, labels = get_files_labels(path + 'train_labels.txt')             # Save all labels from dataset
unique, counts = np.unique(labels, return_counts=True)              # Count the number of times each class occurs in dataset 


colors = ["blue","#0030ff","#005fff","#008fff","DeepSkyBlue"]       # Define a color pallete

numbers = list(sorted(set(counts)))[::-1]                           # Keep only distinct values for occurrences and sort them in descending order

col = []                                                            # Initialize a color list

# Match each class with the same number of occurrences to the same color. The darker the blue is the bigger the number of occurences

for val in counts:                                                  
    for i in range(len(numbers)):                                   
        if val == numbers[i]:
            col.append(colors[i])

plot_histogram(unique,counts,colors=col)                            # Plot histogram
plt.rcParams['font.size'] = '10'                                    # Define parameter size

_, labels = get_files_labels(path + 'train_labels.txt', class_mapping = class_mapping)   # Save all labels after the class mapping
unique, counts = np.unique(labels, return_counts=True)                                   # Count the number of times each class occurs in dataset 
 
colors = ["blue","#0030ff","#005fff","#008fff","DeepSkyBlue"]                            # Define a color pallete

numbers = list(sorted(set(counts)))[::-1]                                                # Keep only distinct values for occurrences and sort them in descending order

col = []                                                                                 # Initialize a color list
  
# Match each class with the same number of occurrences to the same color. The darker the blue is the bigger the number of occurences
for val in counts:
    for i in range(len(numbers)):
        if val == numbers[i]:
            col.append(colors[i])

plot_histogram(unique,counts,colors=col)                                                 # Plot histogram
plt.rcParams['font.size'] = '10'                                                         # Define parameter size

"""### Βήμα 5: Αναγνώριση μουσικού είδους με Long Short-Term Memory (LSTM) Network"""

import torch
import torch.nn as nn

class BasicLSTM(nn.Module):
    
    def __init__(self, input_dim, hidden_dim, num_layers, output_dim, bidirectional=False, dropout = 0):

        super(BasicLSTM, self).__init__()
        self.bidirectional = bidirectional                                       # Parameter that defines if our model is unidirectional or bidiretional (bidirectional if True)
        self.feature_size = hidden_dim*2 if self.bidirectional else hidden_dim   # Feature size

        self.hidden_dim = hidden_dim         # Hidden dimensions
        self.num_layers = num_layers         # Number of hidden layers
        
        self.lstm    = nn.LSTM(input_dim, hidden_dim, num_layers, dropout = dropout, bidirectional=self.bidirectional, batch_first=True)  # Define LSTM model
        self.linear  = nn.Linear(self.feature_size, output_dim) # Apply linear transformation before output

    def forward(self, x, lengths):

        h0 = torch.zeros(self.num_layers*(1+int(self.bidirectional)), x.size(0), self.hidden_dim).double().requires_grad_()           # Initialize hidden state with zeros
        c0 = torch.zeros(self.num_layers*(1+int(self.bidirectional)), x.size(0), self.hidden_dim).double().requires_grad_()           # Initialize cell state
        if torch.cuda.is_available():
            h0 = h0.cuda()
            c0 = c0.cuda()
        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))
        last_outputs = self.last_timestep(out,lengths,self.bidirectional)
        last_outputs = self.linear(last_outputs)
        return last_outputs

    def last_timestep(self, outputs, lengths, bidirectional=False):

        if bidirectional:
            forward, backward = self.split_directions(outputs)
            last_forward = self.last_by_index(forward, lengths)
            last_backward = backward[:, 0, :]
            # Concatenate and return - maybe add more functionalities like average
            return torch.cat((last_forward, last_backward), dim=-1)

        else:
            return self.last_by_index(outputs, lengths)

    @staticmethod
    def split_directions(outputs):

        direction_size = int(outputs.size(-1) / 2)
        forward = outputs[:, :, :direction_size]
        backward = outputs[:, :, direction_size:]
        return forward, backward

    @staticmethod
    def last_by_index(outputs, lengths):

        idx = (lengths - 1).view(-1, 1).expand(outputs.size(0),
                                               outputs.size(2)).unsqueeze(1)
        return outputs.gather(1, idx).squeeze()

def fit(model, train_loader, validation_loader, num_epochs, patience = 10, batch_size = 32, L2 = 0, earlyStopping = True, nn_type = "LSTM"):
    
    # Cross Entropy Loss 
    loss_fn = nn.CrossEntropyLoss()

    # Adam Optimizer
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=L2)
    
    min_val_loss = np.Inf
    
    early_stop = False

    for epoch in range(num_epochs):

        model.train()
        train_loss = 0
        correct = 0
        n_samples = 0
        for batch, labels, lengths in train_loader:
            if torch.cuda.is_available():
                batch = batch.cuda()
                labels = labels.cuda()
                lengths = lengths.cuda()
            # Clear gradients
            optimizer.zero_grad()

            # Calculate output
            if nn_type == "LSTM":
                output = model(batch,lengths)
            else:
                output = model(batch)

            # Calculate cross entropy loss
            loss = loss_fn(output, labels)
            
            predicted = torch.argmax(output, 1)
            correct += (predicted == labels).sum().item() 
            n_samples += len(labels)

            # Calculating gradients
            loss.backward()

            # Update parameters
            optimizer.step()
        
            train_loss += loss.item()
        train_loss /= len(train_loader)
        train_accuracy = correct / n_samples

        model.eval()
        valid_loss = 0
        correct = 0 
        n_samples = 0
        with torch.no_grad():
            for batch, labels, lengths in validation_loader:
                if torch.cuda.is_available():
                        batch = batch.cuda()
                        labels = labels.cuda()
                        lengths = lengths.cuda()
                # Calculate output
                if nn_type == "LSTM":
                    output = model(batch,lengths)
                else:
                    output = model(batch)
                # Calculate cross entropy loss
                loss = loss_fn(output, labels)
                predicted = torch.argmax(output, 1)
                correct += (predicted == labels).sum().item() 
                n_samples += len(labels)
                valid_loss += loss.item()
        valid_loss /= len(validation_loader)
        valid_accuracy = correct / n_samples
        # Check for use of earlyStopping
        if earlyStopping:
            if valid_loss < min_val_loss: # improvement
                torch.save(model.state_dict(),"best.pt")
                epochs_no_improve = 0
                min_val_loss = valid_loss
            else: # no improvement
                epochs_no_improve +=1 
            if epochs_no_improve == patience:
                early_stop = True

        print("Epoch: {}/{}".format(epoch+1,num_epochs), "...", "\033[1mTraining loss: {:.6f}\033[0m".format(train_loss), "...", "Training Accuracy: {:.6f}".format(train_accuracy), "...", "\033[1mValidation loss: {:.6f}\033[0m".format(valid_loss), "...", "Validation Accuracy: {:.6f}".format(valid_accuracy))

        # Check early stopping condition
        if early_stop and earlyStopping:
            print('Early stopping!')
            print('Stopped')
            break

    return

from sklearn.metrics import classification_report

def find_accuracy(model, data_loader, nn_type="LSTM"):
    model.load_state_dict(torch.load("best.pt"))
    model.eval()
    correct = 0
    n_samples = 0
    targets, predictions = [], []
    with torch.no_grad():
            for batch, labels, lengths in data_loader:
                if torch.cuda.is_available():
                        batch = batch.cuda()
                        labels = labels.cuda()
                        lengths = lengths.cuda()
                if nn_type == "LSTM":
                    output = model(batch,lengths)
                else:
                    output = model(batch)
                predicted = torch.argmax(output,1)
                predictions.extend(predicted.cpu().numpy())
                targets.extend(labels.cpu().numpy())
                correct += (predicted == labels).sum().item() 
                n_samples += len(labels)
    accuracy = correct / n_samples
    print(classification_report(targets,predictions))
    return accuracy

"""#### (a) Mel spectrograms"""

# Load Mel spectrogram train Dataset

mel_specs = SpectrogramDataset(
         '../input/patreco3-multitask-affective-music/data/fma_genre_spectrograms/',
         train=True,
         class_mapping=class_mapping,
         max_length=-1,
         read_spec_fn=read_mel_spectrogram)

train_loader_mel, val_loader_mel = torch_train_val_split(mel_specs, 32 ,32, val_size=.33)   # Create train and validation loaders for Mel spectrogram Dataset

# Load Mel spectrogram test Dataset   

test_mel_specs = SpectrogramDataset(
         '../input/patreco3-multitask-affective-music/data/fma_genre_spectrograms/',
         train=False,
         class_mapping=class_mapping,
         max_length=-1,
         read_spec_fn=read_mel_spectrogram)

test_loader_mel = DataLoader(test_mel_specs, batch_size=32)                                 # Create test loader for Mel spectrogram Dataset

# Create LSTM
input_dim = 128     # input dimension
hidden_dim = 256   # hidden layer dimension
layer_dim = 2     # number of hidden layers
output_dim = 10   # output dimension

# Number of epochs
num_epochs = 100

model = BasicLSTM(input_dim, hidden_dim, layer_dim, output_dim, bidirectional=True, dropout=0.2).double()

if torch.cuda.is_available():
    model = model.cuda()
    
# Fit
fit(model, train_loader_mel, val_loader_mel, num_epochs, L2 = 0, earlyStopping=True)

print("\033[1m            Classification report for validation dataset\n\033[0m")    # Print Classification report for validation dataset
val_accuracy = find_accuracy(model, val_loader_mel)
print("\n\033[1m               Classification report for test dataset\n\033[0m")     # Print Classification report for test dataset
test_accuracy = find_accuracy(model, test_loader_mel)

"""#### (b) Beat-synced mel spectrograms"""

# Load Beat-synced Mel spectrogram train Dataset

beat_mel_specs = SpectrogramDataset(
         '../input/patreco3-multitask-affective-music/data/fma_genre_spectrograms_beat/',
         train=True,
         class_mapping=class_mapping,
         max_length=-1,
         read_spec_fn=read_mel_spectrogram)
    
train_loader_beat_mel, val_loader_beat_mel = torch_train_val_split(beat_mel_specs, 32 ,32, val_size=.33) # Create train and validation loaders for beat-synced mel spectrogram dataset

# Load Beat-synced Mel spectrogram test Dataset

test_beat_mel_specs = SpectrogramDataset(
         '../input/patreco3-multitask-affective-music/data/fma_genre_spectrograms_beat/',
         train=False,
         class_mapping=class_mapping,
         max_length=-1,
         read_spec_fn=read_mel_spectrogram)

test_loader_beat_mel = DataLoader(test_beat_mel_specs, batch_size=32)                                     # Create test loader for beat-synced mel spectrogram dataset

# Create LSTM
input_dim = 128     # input dimension
hidden_dim = 256   # hidden layer dimension
layer_dim = 2     # number of hidden layers
output_dim = 10   # output dimension

# Number of epochs
num_epochs = 100

model = BasicLSTM(input_dim, hidden_dim, layer_dim, output_dim, bidirectional=True, dropout=0.2).double()

if torch.cuda.is_available():
    model = model.cuda()
    
# Fit
fit(model, train_loader_beat_mel, val_loader_beat_mel, num_epochs, L2 = 0, earlyStopping=True)

print("\033[1m            Classification report for validation dataset\n\033[0m")   # Print Classification report for validation dataset
val_accuracy = find_accuracy(model, val_loader_beat_mel)
print("\n\033[1m               Classification report for test dataset\n\033[0m")    # Print Classification report for test dataset
test_accuracy = find_accuracy(model, test_loader_beat_mel)

"""#### (c) Chromagrams"""

# Load chromagram train dataset

chroma = SpectrogramDataset(
         '../input/patreco3-multitask-affective-music/data/fma_genre_spectrograms/',
         train=True,
         class_mapping=class_mapping,
         max_length=-1,
         read_spec_fn=read_chromagram)

train_loader_chroma, val_loader_chroma = torch_train_val_split(chroma, 32 ,32, val_size=.33)   # Create train and validation loader for chromagram dataset

# Load chromagram test dataset

test_chroma = SpectrogramDataset(
         '../input/patreco3-multitask-affective-music/data/fma_genre_spectrograms/',
         train=False,
         class_mapping=class_mapping,
         max_length=-1,
         read_spec_fn=read_chromagram)

test_loader_chroma = DataLoader(test_chroma, batch_size=32)                                     # Create test loader for chromagram dataset

# Create LSTM
input_dim = 12     # input dimension
hidden_dim = 256   # hidden layer dimension
layer_dim = 2     # number of hidden layers
output_dim = 10   # output dimension

# Number of epochs
num_epochs = 100

model = BasicLSTM(input_dim, hidden_dim, layer_dim, output_dim, bidirectional=True, dropout=0.2).double()

if torch.cuda.is_available():
    model = model.cuda()
    
# Fit
fit(model, train_loader_chroma, val_loader_chroma, num_epochs, L2 = 0, earlyStopping=True)

print("\033[1m            Classification report for validation dataset\n\033[0m")   # Print Classification report for validation dataset
val_accuracy = find_accuracy(model, val_loader_chroma)
print("\n\033[1m               Classification report for test dataset\n\033[0m")    # Print Classification report for test dataset
test_accuracy = find_accuracy(model, test_loader_chroma)

"""#### (d) Beat-synced chromagrams"""

# Load beat-synced chromagram train dataset

beat_chroma = SpectrogramDataset(
         '../input/patreco3-multitask-affective-music/data/fma_genre_spectrograms_beat/',
         train=True,
         class_mapping=class_mapping,
         max_length=-1,
         read_spec_fn=read_chromagram)

train_loader_beat_chroma, val_loader_beat_chroma = torch_train_val_split(beat_chroma, 32 ,32, val_size=.33)  # Create train and validation loader for beat-synced chromagram dataset

# Load beat-synced chromagram test dataset

test_beat_chroma = SpectrogramDataset(
         '../input/patreco3-multitask-affective-music/data/fma_genre_spectrograms_beat/',
         train=False,
         class_mapping=class_mapping,
         max_length=-1,
         read_spec_fn=read_chromagram)

test_loader_beat_chroma = DataLoader(test_beat_chroma, batch_size=32)                                        # Create test loader for beat-synced chromagram dataset

# Create LSTM
input_dim = 12     # input dimension
hidden_dim = 256   # hidden layer dimension
layer_dim = 2     # number of hidden layers
output_dim = 10   # output dimension

# Number of epochs
num_epochs = 100

model = BasicLSTM(input_dim, hidden_dim, layer_dim, output_dim, bidirectional=True, dropout=0.2).double()

if torch.cuda.is_available():
    model = model.cuda()
    
# Fit
fit(model, train_loader_beat_chroma, val_loader_beat_chroma, num_epochs, L2 = 0, earlyStopping=True)

print("\033[1m            Classification report for validation dataset\n\033[0m")   # Print Classification report for validation dataset
val_accuracy = find_accuracy(model, val_loader_beat_chroma)
print("\n\033[1m               Classification report for test dataset\n\033[0m")    # Print Classification report for test dataset
test_accuracy = find_accuracy(model, test_loader_beat_chroma)

"""#### Bonus"""

# Load fused spectrogram train dataset

beat_fused = SpectrogramDataset(
         '../input/patreco3-multitask-affective-music/data/fma_genre_spectrograms_beat/',
         train=True,
         class_mapping=class_mapping,
         max_length=-1,
         read_spec_fn=read_fused_spectrogram)

train_loader_beat_fused, val_loader_beat_fused = torch_train_val_split(beat_fused, 32 ,32, val_size=.33) # Create train and validation loader for fused spectorgram dataset

# Load fused spectrogram test dataset

test_beat_fused = SpectrogramDataset(
         '../input/patreco3-multitask-affective-music/data/fma_genre_spectrograms_beat/',
         train=False,
         class_mapping=class_mapping,
         max_length=-1,
         read_spec_fn=read_fused_spectrogram)

test_loader_beat_fused = DataLoader(test_beat_fused, batch_size=32)                                       # Create test loader for fused spectorgram dataset

# Create LSTM
input_dim = 140    # input dimension
hidden_dim = 256   # hidden layer dimension
layer_dim = 2     # number of hidden layers
output_dim = 10   # output dimension

# Number of epochs
num_epochs = 100

model = BasicLSTM(input_dim, hidden_dim, layer_dim, output_dim, bidirectional=True, dropout=0.2).double()

if torch.cuda.is_available():
    model = model.cuda()
    
# Fit
fit(model, train_loader_beat_fused, val_loader_beat_fused, num_epochs, L2 = 0, earlyStopping=True)

print("\033[1m            Classification report for validation dataset\n\033[0m")   # Print Classification report for validation dataset
val_accuracy = find_accuracy(model, val_loader_beat_fused)
print("\n\033[1m               Classification report for test dataset\n\033[0m")    # Print Classification report for test dataset
test_accuracy = find_accuracy(model, test_loader_beat_fused)

"""### Βήμα 7: 2D CNN"""

class ConvNet(nn.Module):
    
    def __init__(self, in_channels=1, out_channels=32, output_dim=10):
        
        super(ConvNet,self).__init__()
        self.conv1 = nn.Sequential(                                                          # Define a sequential architecture 
            nn.Conv2d(in_channels, out_channels, kernel_size=3,padding=1),                   # 1st Convolution layer
            nn.BatchNorm2d(out_channels),                                                    # 1st Batch normalization layer
            nn.ReLU(),                                                                       # ReLU activation function
            nn.MaxPool2d(kernel_size=2))                                                     # Max Pooling with kernel size = 2
        
        self.conv2 = nn.Sequential(                                                          # Define a sequential architecture 
            nn.Conv2d(out_channels, out_channels*2, kernel_size=3,padding=1),                # 2nd Convolution layer
            nn.BatchNorm2d(out_channels*2),                                                  # 2nd Batch normalization layer
            nn.ReLU(),                                                                       # ReLU activation function  
            nn.MaxPool2d(kernel_size=2))                                                     # Max Pooling with kernel size = 2
        
        self.conv3 = nn.Sequential(                                                          # Define a sequential architecture 
            nn.Conv2d(out_channels*2, out_channels*3, kernel_size=3,padding=1),              # 3rd Convolution layer
            nn.BatchNorm2d(out_channels*3),                                                  # 3rd Batch normalization layer
            nn.ReLU(),                                                                       # ReLU activation function   
            nn.MaxPool2d(kernel_size=2))                                                     # Max Pooling with kernel size = 2
        
        self.conv4 = nn.Sequential(                                                          # Define a sequential architecture 
            nn.Conv2d(out_channels*3, out_channels*4, kernel_size=3, stride=1, padding=1),   # 4th Convolution layer
            nn.BatchNorm2d(out_channels*4),                                                  # 4th Batch normalization layer
            nn.ReLU(),                                                                       # ReLU activation function 
            nn.MaxPool2d(kernel_size=2))                                                     # Max Pooling with kernel size = 2
        
        self.linear1 = nn.Linear(81920,512)                                                  # 1st linear layer
        self.linear2 = nn.Linear(512,output_dim)                                             # 2nd linear layer

        
    def forward(self, x):
        
        x = x.view(x.size(0), 1, x.size(1), x.size(2))
        x = self.conv1(x)
        x = self.conv2(x)
        x = self.conv3(x)
        x = self.conv4(x)
        
        x = x.reshape(x.size(0),-1)
        x = self.linear1(x)
        x = self.linear2(x)
        return x

# Create ConvNet
in_channels = 1     # number of channels
out_channels = 32   # hidden layer dimension
output_dim = 10     # output dimension

# Number of epochs
num_epochs = 100

model = ConvNet(in_channels, out_channels, output_dim).double()

if torch.cuda.is_available():
    model = model.cuda()

# Fit
fit(model, train_loader_mel, val_loader_mel, num_epochs, L2 = 0, earlyStopping=True, nn_type="CNN")

val_accuracy = find_accuracy(model, val_loader_mel, nn_type="CNN")
test_accuracy = find_accuracy(model, test_loader_mel, nn_type="CNN")

print("Accuracy of the best model on \033[1mvalidation data\033[0m is \033[1m{}%\033[0m".format(np.round(val_accuracy*100,3))) # Print Classification report for validation dataset
print("Accuracy of the best model on \033[1mtest data\033[0m is \033[1m{}%\033[0m".format(np.round(test_accuracy*100,3)))      # Print Classification report for test dataset